# Archivo Markdown - Entregable

## Nombre del Proyecto: Proyecto Final ETL


## -- Project Status: [Completed]

## Miembros del equipo

Team Leader: [Daniel Martinez]()(https://github.com/Daniel1martinez2) Instructor: [Angela Villota]

## Other Members:

Name	Email
[Alejandra Ruiz](https://github.com/alejaguz)	@alejaguz
[Luis Felipe Montenegro](https://github.com/felipe-montenegro)	@felipe-montenegro
[Juan Camilo Vergara](https://github.com/juancamilovergaraarenas)	@juancamilovergaraarenas



## Contact

Team Leader: [Daniel Martinez]()(https://github.com/Daniel1martinez2)


## Introducci√≥n/Objetivo del Proyecto

Construir el proceso ETL utilizando Pentaho Data Integration (PDI) con el fin de obtener un archivo csv, realizando el proceso de extracci√≥n y transformaci√≥n de los datos disponibles en la base de datos relacional BD Ames, en archivos csv y en MongoDB. 


## M√©todos Utilizados

- PgAdmin para bases de datos relacionales (PostGres)
- MongoDB para documentos
- Pentaho para integraci√≥n de Bases


## Descripci√≥n del Proyecto

1. Archivo de Inmobiliaria (BASE RELACIONAL)

![alt text](Images/image.png)

- Se carga el script para crear el DataBase y se crean las tablas vacias
- Se cargan los scripts de cada tabla y se cargan en el siguiente orden: primero, las que no tienen relaciones con otras tablas para evitar error de relaciones que puedan darse con tablas que no existan

Orden (se adjuntan algunos recortes de ejemplo):
- TypeQuality primero porque es base de otras tablas
- Mszoning segundo porque tambi√©n es una tabla base de otras
- Msubclass tercero porque tambi√©n es una tabla base de otras
 
 ![alt text](Images/image-1.png)

- Amsdbtemp es la cuarta

 ![alt text](Images/image-2.png)

- Salesproperty es la quinta

 ![alt text](Images/image-3.png)

- Floordetail es la √∫ltima por cargar

 ![alt text](Images/image-4.png)


Luego de hacer esto entonces revisamos el diagrama relacional resultante:

 ![alt text](Images/image-5.png)

En este caso nos damos cuenta que no coincide con el diagrama relacional real y por ende revisamos el c√≥digo de los scripts y no encontramos una relaci√≥n creada. As√≠ que procedemos a completar el c√≥digo para generar las relaciones pertinentes.

Se crea entonces las relaciones de typequality, mszoning y mssubclass:

 ![alt text](Images/image-6.png)


Diagrama resultante del c√≥digo:

 ![alt text](Images/image-7.png)





2. Carga Base de Datos 2: Infrati Test - MongoDB

 
![alt text](Images/image-8.png)


3. Carga de Base de Datos relacional BD Ames a Pentaho 

![alt text](Images/image-9.png)

4. Antes de llevar a cabo el Join completo de la base de datos relacional, se lleva a cabo el c√°lculo de las variables FullBath, HalfBath y Bedroom

![alt text](Images/image-24.png)

Adicionalmente, se hizo la modificaci√≥n de Fecha de venta, asegurando que en la salida solo est√© el mes y el a√±o de venta

![alt text](Images/image-44.png)

5. Join de bases de datos 

Luego para practicidad del ejercicio entonces se realiz√≥ el join de la mayor√≠a de tablas excepto amesdbtemp debido a que esta base de datos tiene muchas variables

 ![alt text](Images/image-14.png)

Luego se termin√≥ de unificar la tabla de la base relacional con amesdbtemp para construir el primer merge y luego crear el merge completo:

 ![alt text](Images/image-15.png)


6. Carga de CSV
 

![alt text](Images/image-12.png)


7. Paso intermedio - Transformaciones

Para poder realizar las transformaciones de las bases de datos, realizamos un barrido donde identificamos la fuente y nombres de variables de la salida esperada, de la siguiente manera (donde dejamos en amarillo aquellas que requieren un c√°lculo):

![alt text](Images/image-13.png)





8. Se lleva al c√°lculo de la variable gr_live_area utilizando el calculator
 
![alt text](Images/image-16.png)

![alt text](Images/image-17.png)

![alt text](Images/image-25.png)

9. Se lleva a cabo el merge con la base de datos CSV Property

![alt text](Images/image-23.png)


10. Se llev√≥ a cabo la transformaci√≥n solicitada para la variable Year_Remod/Add

- Se filtra la informaci√≥n en donde la varibale Year_Remod/Add es nula

![alt text](Images/image-27.png)

- Luego, aquellas celdas nulas se imputaron con el contenido de la variable Year_built

![alt text](Images/image-28.png)

- Luego se hizo la uni√≥n entre los imputados y los que quedaron fuera del primer filtro

![alt text](Images/image-29.png)


11. En este paso se llev√≥ a cabo la carga de las bases de datos de mongo:

- Bsmt
- Garage
- Misc
- Pool

Para cada una realizamos tres operaciones: la carga, el sort y el merge con las otras bases: 

- La carga

![alt text](Images/image-30.png)

![alt text](Images/image-33.png)

- El sort

![alt text](Images/image-31.png)

- El merge

![alt text](Images/image-32.png)

*Importante fue deseleccionar el output single JSON field, para que se transformara en formato de tabla y asi si se pudiera realizar el Merge. Para este caso, fue importante que se realizara un left outer join para que se hiciera el merge correctamente, porque las bases de mongo ten√≠an menos valores y por eso quer√≠amos conservar la tabla con mayor cantidad de datos y hacer el join con los menores valores de mongo.

 se valid√≥ que no existieran duplicados a la hora de realizar el Merge


![alt text](Images/image-20.png)

Al final de este proceso, las bases de mongo quedaron as√≠:

![alt text](Images/image-34.png)

12. Luego de realizar la carga de los datos de MongoDB, se lleva a cabo la imputaci√≥n de datos faltantes de la siguiente manera: NA si es cualitativa y 0 si la variable es n√∫merica

![alt text](Images/image-35.png)

![alt text](Images/image-36.png)

13. Luego se llev√≥ a cabo la transformaci√≥n de la variable Neighborhood. En primer lugar se cre√≥ un archivo CSV que luego fue imputado al pentaho

![alt text](Images/image-37.png)

![alt text](Images/image-38.png)

14. Posterior a esto se llev√≥ a cabo el merge entre la base de datos resultante despu√©s de realizar todo el proceso de merge de las bases de datos de mongo con el CSV cargado

![alt text](Images/image-39.png)

15. A continuaci√≥n se lleva  acabo la transformaci√≥n de la variable LotShape utilizando la funci√≥n value mapper

![alt text](Images/image-40.png)

16. Para finalizar el proceso de tranformaci√≥n y para asegurar la calidad de la salida se hizo una selecci√≥n de valores en la que se realizaron los siguientes cambios:

- Se eliminaron columnas duplicadas


- Se renombr√≥ la variable Bedroom por Bedroom AbvGr

![alt text](Images/image-41.png)

- Se renombr√≥ la variable Code por Neighborhood

![alt text](Images/image-42.png)

17. Finalmente se export√≥ el archivo definivo utilizando la funci√≥n test_file_output

![alt text](Images/image-43.png)

18. AsÌ quedÛ el pipeline de todo el proceso:
![alt text](Images/Screenshot.png)

## Lecciones aprendidas:

- Es importante mencionar que a lo largo del flujo de trabajo siempre se llev√≥ a cabo un proceso de ordenamiento de los datos previo a cada merge, esto con el objetivo de asegurar la calidad de los datos evitando duplicados, errores y buscanod mantener relaciones correctas entre las tablas.

- Con el prop√≥sito de reducir el exceso de pasos y  procesamientos en la herramienta Pentaho, es relevante optimizar las queries realizadas en el preprocesamientos de la carga inicial de los datos

- Reforzar que el n√∫mero de filas esperadas depende de la cantidad de IDS que se tengan en total de todas las fuentes. 


